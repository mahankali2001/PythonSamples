{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to Vijay NLP Tutorials.\n",
      "Please do watch the entire session, to become expert in NLP.\n",
      "Hello\n",
      "Welcome\n",
      ",\n",
      "to\n",
      "Vijay\n",
      "NLP\n",
      "Tutorials\n",
      ".\n",
      "Please\n",
      "do\n",
      "watch\n",
      "the\n",
      "entire\n",
      "session\n",
      ",\n",
      "to\n",
      "become\n",
      "expert\n",
      "in\n",
      "NLP\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vmahankali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vmahankali/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Tokenization\n",
    "## Corpus -> Documents or Sentence -> Paragraphs\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Verify and create NLTK data directory if it doesn't exist\n",
    "nltk_data_dir = '/Users/vmahankali/nltk_data'\n",
    "if not os.path.exists(nltk_data_dir):\n",
    "    os.makedirs(nltk_data_dir)\n",
    "\n",
    "# Set the NLTK data directory\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Download the Punkt tokenizer models\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_dir)\n",
    "\n",
    "# # Debugging: Check NLTK data path\n",
    "# print(\"NLTK data path:\", nltk.data.path)\n",
    "\n",
    "# # Debugging: Check if Punkt tokenizer models are present\n",
    "# punkt_path = os.path.join(nltk_data_dir, 'tokenizers', 'punkt')\n",
    "# print(\"Punkt tokenizer path:\", punkt_path)\n",
    "# print(\"Punkt tokenizer files:\", os.listdir(punkt_path) if os.path.exists(punkt_path) else \"Not found\")\n",
    "\n",
    "corpus=\"\"\"Hello Welcome, to Vijay NLP Tutorials. \n",
    "Please do watch the entire session, to become expert in NLP.\"\"\"\n",
    "\n",
    "documents=sent_tokenize(corpus, language=\"english\")\n",
    "# print(documents)\n",
    "# type (documents)\n",
    "\n",
    "for sentence in documents:\n",
    "    print(sentence)\n",
    "\n",
    "\n",
    "words=word_tokenize(corpus, language=\"english\")\n",
    "\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Vijay',\n",
       " 'NLP',\n",
       " 'Tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'session',\n",
       " ',',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalize---->final\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalize\"]\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(word + \"---->\" + stemming.stem(word))\n",
    "\n",
    "# history---->histori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regstemming = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
